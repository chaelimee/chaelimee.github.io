---
layout: post
title:  "[인공지능 명예학회] 2nd_week 이론 정리"
date:   2023-11-28 14:04:36 +0900
categories: sample
published: true
---
# 수정중
## 2nd_week 이론 수업 요약

### 시그모이드 함수 (sigmoid)
* 결과값을 0~1 사이의 값으로 나타낸다 (0~1값 이외에는 불가)
* 함수의 출력이 0.5보다 크면 양성 클래스, 0.5보다 작으면 음성 클래스로 판단 * 로지스틱 == 시그모이드

<br>

### softmax 함수
* 시그모이드 함수와 비슷하게 e값이 들어감 
* 출력 결과를 정규화하여 합이 1이 되도록함 
* 다중 클래스 분류 문제에서 사용

<br>

### 데이터 스케일링
: 데이터의 특성들을 일정한 범위로 조정하는 과정
* 표준화 (Standardization) 
    * 특성의 평균을 0으로, 표준편차를 1로 만들어 데이터를 정규 분포에 가깝게 만듦
* 최대-최소 정규화 (Min-Max Scaling) 
    * 모든 특성의 값을 일정한 최솟값, 최댓값 사이로 변환
    * 최솟값은 0, 최댓값은 1로 설정하는 경우가 많음.
* 벡터 정규화 (Normalizer) 
    * 주로 벡터의 크기를 조절하는 목적으로 사용되는 특정한 변환 방법
    * 예를 들어 L1 노름 또는 L2 노름을 사용하여 정규화를 수행할 수 있음

<br>

### 손실함수
: 확률적 경사 하강법이 최적화할 대상 
* MSE(Mean Squared Error) : 평균제곱오차 
* RMSE(Root Mean Squared Error)
* Binary Crossentropy : 이진 크로스엔트로피

<br>

### 경사 하강법 (Gradient Descent)
: 손실값을 최소화하기 위한 알고리즘
* Batch Gradient Descent: 전체 train셋 사용. (메모리 요구가 큼) 
* SGD(stochastic Gradient Descent): 각 단계 무작위 선택된 train 셋
* Mini-batch Gradient Descent: 배치 단위 무작위 train 셋

<br>

### Epoch란?
: 훈련 횟수를 의미함

<br>

### 결정 트리 (Decision Tree)
: 예/아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘 <br>
비교적 예측 과정을 이해하기 쉽고 성능도 뛰어남->그래서 한글로 의사결정 트리라고 표현함

<br>

### 불순도 (Impurity)
: 결정 트리가 최적의 질문을 찾기 위한 기준<br>
불순도가 낮을수록 데이터가 순수하다고 판단<br>
주어진 데이터의 클래스가 섞여있을수록 불순도가 높아짐<br>

<br>

### Cross Validation
: 성능 평가 방식의 신뢰성을 보기 위한 것 <br>
데이터를 여러 번 나눠 모델을 학습하고 평가하는 과정을 반복함

<br>

### 하이퍼파라미터 (Hyperparameter)
: 모델을 학습하기 전에 직접 지정해야하는 조절 가능한 매개변수 <br>
모델의 구조나 학습 방법을 결정함 >> 값을 조절하여 모델의 성능을 향상시킬 수 있음